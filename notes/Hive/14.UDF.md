# 自定义 UDF 函数

## 1.UDF

（1）函数功能实现

```java
package udf;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/*
	函数功能：将字符串转小写
 */
public class MyLower extends GenericUDF {

    /**
     * 输入参数类型的鉴别器对象
     * @param arguments
     * @return
     * @throws UDFArgumentException
     */
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
        // 判断输入参数的个数
        if (arguments.length != 1) {
            throw new UDFArgumentLengthException(
                    "LOWER requires 1 argument, got " + arguments.length);
        }

        // 判断输入参数的类型
        if (arguments[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
            throw new UDFArgumentException("LOWER only takes primitive types");
        }

        //函数本身返回值为 Sting ，需要返回 Sting 类型的鉴别器对象
        return PrimitiveObjectInspectorFactory.javaStringObjectInspector;
    }

    /**
     * 函数实现逻辑
     * @param deferredObjects 输入的参数
     * @return
     * @throws HiveException
     */
    @Override
    public Object evaluate(DeferredObject[] deferredObjects) throws HiveException {
        String val = deferredObjects[0].get().toString();
        if (val == null)
            return null;
        // 返回字符串的小写形式
        return val.toLowerCase();
    }

    /**
     * Get the String to be displayed in explain.
     */
    @Override
    public String getDisplayString(String[] strings) {
        return null;
    }
}
```

（2）打成jar包，并上传到服务器；

（3）将 jar 包添加到 hive 的 classpath

```shell
hive(default)> add jar /opt/module/data/myudf.jar;
```

如果我们将jar包上传到hive的lib目录，则不需要这个步骤。因为，hive在启动的时候自动扫描lib下所有的jar包。

但是，lib是静态目录，只有在hive启动的时候才会加载。所以，即时将jar包放在lib目录下，不适用add jar的话，只能等hive再次启动的时候才能扫描到新增的jar包。

（4）创建临时函数与开发好的 java class 关联

```java
hive (default)> create temporary function my_low as "udf.MyLower";
```

（5）在hive中使用自定义的函数

```sql
select
	name
	,my_low(name) as low_name
from table
```



## 2.UDTF

```java
/*
    实现split函数功能
 */
public class MyUDTF extends GenericUDTF {
    private ArrayList<String> outList = new ArrayList<>();

    @Override
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {
        // 1.定义输出数据的列名和类型
        List<String> names = new ArrayList<>();
        List<ObjectInspector> fieldOIs = new ArrayList<>();

        // 2.添加输出数据的列名和类型
        names.add("lineToWord");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory
                .getStandardStructObjectInspector(names, fieldOIs);
    }

    @Override
    public void process(Object[] args) throws HiveException {
        //1.获取原始数据
        String arg = args[0].toString();

        //2.获取数据传入的第二个参数，此处为分隔符
        String splitKey = args[1].toString();

        //3.将原始数据按照传入的分隔符进行切分
        String[] fields = arg.split(splitKey);

        //4.遍历切分后的结果，并写出
        for (String field : fields) {
            //集合为复用的，首先清空集合
            outList.clear();
            //将每一个单词添加至集合
            outList.add(field);
            //将集合内容写出
            forward(outList);
        }
    }

    @Override
    public void close() throws HiveException {

    }
}
```



